{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deferred Correction Schemes for Ordinary Differential Equations\n",
    "\n",
    "In this notebook we will see that we can use the Picard integral formulation of an ordinary differential equation (ODE) to derive an equation for the error in our numerical approximation. This error equation can also be written as a Picard integral equation which means that we can solve it using the same method we used to solve the original equation. This provides a way to 'correct' our original approximation. We will apply this method to the Dahlquist equation and show that successive corrections increase the order of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We will consider scalar ODEs defined on a time interval $t\\in[t_0, t_N]$:\n",
    "\n",
    "\\begin{align}\\label{ode}\n",
    "y'(t) &= f(y(t), t)\\\\\n",
    "y(t_0) &= y_0\n",
    "\\end{align}\n",
    "\n",
    "The first step is to recast the equation into its integral form:\n",
    "\n",
    "\\begin{equation}\\label{picard}\n",
    "y(t) = y_0 + \\int_{t_0}^t f(y(\\tau), \\tau) d\\tau\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "The **error**, denoted by $\\delta$, is the difference between our numerical approximation $\\tilde{y}$ and the solution $y$, which we may not be able to write down in closed form, or indeed at all.\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta(t) = y(t) - \\tilde{y}(t)\n",
    "\\end{equation}\n",
    "\n",
    "The **residual**, denoted by $\\epsilon$, tells us about how well our approximation satisfies the original equation. It has the advantage that it can be calculated without knowledge of the solution, whereas the error cannot. We calculate the residual by substituting out numerical approximation into the equation and rearranging:\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon(t) = y_0 + \\int_{t_0}^t f(\\tilde{y}(\\tau), \\tau) d\\tau - \\tilde{y}(t)\n",
    "\\end{equation}\n",
    "\n",
    "The **order** of a numerical approximation tells us how the error in our numerical approximation changes as our discretisation parameter changes. In this notebook we are solving an initial value problem; that is, we're starting from an initial value $y_0$ and calculating values $y_n$ at successive times $t_n$. The difference $\\Delta t = t_{n+1}-t_n$ is called the timestep (and it may not be constant throughout the time interval). If the error is bounded by\n",
    "\n",
    "\\begin{equation}\n",
    "||\\delta(t)|| = ||y(t) - \\tilde{y}(t)|| \\le C (\\Delta t) ^p,\n",
    "\\end{equation}\n",
    "\n",
    "where $C$ is a parameter-independent constant, we say that the method is of order $p$.\n",
    "\n",
    "For notational clarity (there will be a lot of indices!) from now on we will suppress the dependence of $y$ on $t$ and write $f(y, t)$ for $f(y(t), t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The error equation\n",
    "\n",
    "In this section we will derive an equation for the error that has the same integral form as the origingal equation. The aim is to write the error (which we cannot directly calculate for the general case) in terms of the residual (which we can always calculate).\n",
    "\n",
    "We start with our definition of the error, $\\delta$, substitute from the Picard integral equation for $y$, and rearrange to get\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{y} = y_0 + \\int_{t_0}^t f(y, \\tau) d\\tau - \\delta.\n",
    "\\end{equation}\n",
    "\n",
    "Substituting this in to the definition of the residual, $\\epsilon$, gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon = \\int_{t_0}^t f(\\tilde{y}, \\tau) - f(y, \\tau) d\\tau + \\delta\n",
    "\\end{equation}\n",
    "\n",
    "Rearranging for $\\delta$ gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta = \\epsilon + \\int_{t_0}^t f(y, \\tau) - f(\\tilde{y}, \\tau) d\\tau,\n",
    "\\end{equation}\n",
    "\n",
    "but note that the right hand side includes a term that requires us to evaluate $f$ using the solution $y$. However, we can substitute from the definition of the error to get\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta = \\epsilon + \\int_{t_0}^t f(\\tilde{y}+\\delta, \\tau) - f(\\tilde{y}, \\tau) d\\tau,\n",
    "\\end{equation}\n",
    "\n",
    "which is in the same form as the Picard integral equation for $y$ and can be solved using the same methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the error equation\n",
    "\n",
    "Consider the Picard equation for the error, $\\delta$. The only difference between this equation and the Picard equation formulation of the original ODE is that we need to calculate the residual $\\epsilon$ since this a function of time. Applying the the forward Euler method to the Picard equation for $\\delta$ gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{n+1} = \\delta^n + \\Delta t \\big(G(\\tilde{y}^n, \\delta^n, t^n)\\big) + \\epsilon^{n+1} - \\epsilon^n,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "G(\\tilde{y}, \\delta, t) = f(\\tilde{y}+\\delta, t) - f(\\tilde{y}, t),\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon^{n+1} - \\epsilon^n = \\int_{t^n}^{t^{n+1}} f(\\tilde{y}, \\tau) d\\tau - \\tilde{y}^{n+1} + \\tilde{y}^n.\n",
    "\\end{equation}\n",
    "\n",
    "We see from this that calculating $\\epsilon$ requires us to calculate an integral. We do this numerically, approximating the integral as a sum of terms comprised of weights multiplied by function evaluations, that is\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{t_n}^{t_{n+1}} f(\\tilde{y}, \\tau) d\\tau \\approx \\sum_{m=1}^{M} w_{m,n} f(\\tilde{y}_m, t_m),\n",
    "\\end{equation}\n",
    "\n",
    "where $t_m\\in [t_n, t_{n+1}] \\, \\forall m\\in [1, M]$. We will define the weights $\\{w_{m,n}\\}$ using the Lagrange interpolating polynomial $l_m$ and we choose the $\\{t_m\\}$ to be equispaced, although there are better choices for stability.\n",
    "\n",
    "\\begin{equation}\n",
    "w_{m,n} = \\int_{t_n}^{t_{n+1}}l(\\tau) d\\tau, \\quad n=1, 2, ..., N, \\quad m=1, 2, ..., M\n",
    "\\end{equation}\n",
    "\n",
    "where $l_m(t)$ is the Lagrange interpolating polynomial defined by\n",
    "\n",
    "\\begin{equation}\n",
    "l_m(t) = \\frac{1}{c_m}\\prod_{k=1, k\\ne m}^M (t - t_k), \\quad c_m = \\prod_{k=1, k\\ne m}^M(t_m - t_k).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the functions required to evaluate the weights and form the integration matrix $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equidistant_nodes(M):\n",
    "    # This returns a grid of M equispaced nodes from -1 to 1\n",
    "    grid = np.linspace(-1., 1., M)\n",
    "    return grid\n",
    "\n",
    "def lagrange_polynomial(index, nodes):\n",
    "    # This returns the coefficients of the Lagrange polynomial l_m with m=index\n",
    "\n",
    "    M = len(nodes)\n",
    "\n",
    "    # c is the denominator\n",
    "    c = 1.\n",
    "    for k in range(M):\n",
    "        if k != index:\n",
    "            c *= (nodes[index] - nodes[k])\n",
    "\n",
    "    coeffs = np.zeros(M)\n",
    "    coeffs[0] = 1.\n",
    "    m = 0\n",
    "\n",
    "    for k in range(M):\n",
    "        if k != index:\n",
    "            m += 1\n",
    "            d1 = np.zeros(M)\n",
    "            d2 = np.zeros(M)\n",
    "            \n",
    "            d1 = (-1.)*nodes[k] * coeffs\n",
    "            d2[1:m+1] = coeffs[0:m]\n",
    "                       \n",
    "            coeffs = d1+d2\n",
    "    \n",
    "    return coeffs / c\n",
    "\n",
    "def integrate_polynomial(p):\n",
    "    # given a list of coefficients of a polynomial p, this returns those of the integral of p\n",
    "    integral_coeffs = np.zeros(len(p)+1)\n",
    "    \n",
    "    for n, pn in enumerate(p):\n",
    "        integral_coeffs[n+1] = 1/(n+1) * pn\n",
    "\n",
    "    return integral_coeffs\n",
    "\n",
    "def evaluate(p, a, b):\n",
    "    # given a list of coefficients of a polynomial p, this returns the value of p(b)-p(a)\n",
    "    value = 0.\n",
    "    for n, pn in enumerate(p):\n",
    "        value += pn * (b**n - a**n)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def lagrange_integration_matrix(M):\n",
    "    # using the functions defined above, this returns the MxM integration matrix\n",
    "    \n",
    "    # set up equidistant nodes and initialise matrix to zero\n",
    "    nodes = equidistant_nodes(M)\n",
    "    L = len(nodes)\n",
    "    int_matrix = np.zeros((L, L))\n",
    "    \n",
    "    # fill in matrix values\n",
    "    for index in range(L):       \n",
    "        coeff_p = lagrange_polynomial(index, nodes)\n",
    "        int_coeff = integrate_polynomial(coeff_p)\n",
    "        \n",
    "        for n in range(L-1):\n",
    "            int_matrix[n+1, index] = evaluate(int_coeff, nodes[n], nodes[n+1])\n",
    "    \n",
    "    return int_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use forward Euler as our base method and we will begin by applying SDC to the Dahlquist equation. We will use the functions below - note that the forward Euler function can be used with any right hand side functions and any arguments required by the right hand side function can be passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_euler(x, dt, rhs, *args):\n",
    "    # returns the rhs of the forward Euler approximation with timestep dt to x'=f(x) with f given\n",
    "    # by the function rhs which can take in args\n",
    "    return x + dt*rhs(x, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dahlquist(x, lamda=1):\n",
    "    # returns the rhs of the Dahlquist equation\n",
    "    return lamda*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_solution(x0, lamda, grid):\n",
    "    # returns the exact solution of the Dahlquist equation with initial condition x0 on the provided gri\n",
    "    return x0 * np.exp(lamda*grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can picture the SDC method as applying a timestepper to N groups of M nodes. We iterate over each group of M nodes, first computing the initial solution then updating this by solving the error equation. Once this iteration is complete, we move on to the next set of M nodes.\n",
    "\n",
    "With forward Euler as our base method, we can compute the $k+1^{th}$ iteration as\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{n+1}^{k+1} &= \\tilde{y}_{n+1}^k + \\delta_{n+1} \\\\\n",
    "&= \\tilde{y}_{n+1}^k + \\delta_n + \\Delta t \\big(G(\\tilde{y}_n, \\delta_n, t_n)\\big) + \\epsilon_{n+1} - \\epsilon_n \\\\\n",
    "&= \\tilde{y}_{n+1}^k + \\delta_n + \\Delta t \\big(f(\\tilde{y}_n+\\delta_n, t) - f(\\tilde{y}_n, t)\\big) - \\tilde{y}_{n+1}^k + \\tilde{y}_n^k + \\sum_{m=1}^{M} w_{m,n} f(\\tilde{y}_m, t_m) \\\\\n",
    "&= \\tilde{y}_n^{k+1} + \\Delta t \\big(f(\\tilde{y}_n+\\delta_n, t) - f(\\tilde{y}_n, t)\\big) + \\sum_{m=1}^{M} w_{m,n} f(\\tilde{y}_m, t_m)\n",
    "\\end{align*}\n",
    "\n",
    "The function below performs the iteration over M nodes, i.e. on the finer grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc_fine_step(yn, M, dt, rhs):\n",
    "    # yn is the value at the first node in the group\n",
    "    # M is the number of nodes in the group\n",
    "    # dt is the timestep on this finer grid\n",
    "    # nits is the number of iterations\n",
    "    # rhs is the function to evaluate the rhs of the equation we are solving\n",
    "\n",
    "    # time grid\n",
    "    grid = np.arange(0., M*dt, dt)\n",
    "    \n",
    "    # old and iterated solution\n",
    "    old_sol = np.empty(M)\n",
    "    new_sol = np.empty(M)    \n",
    "    old_sol[0] = yn\n",
    "    new_sol[0] = yn\n",
    "    \n",
    "    # Integration matrix\n",
    "    integration_matrix = lagrange_integration_matrix(M)\n",
    "    # Rescale integration matrix because nodes defined on [-1, 1]\n",
    "    integration_matrix = 0.5 * (M-1) * dt * integration_matrix\n",
    "    \n",
    "    # Compute initial guess\n",
    "    for n in range(M-1):\n",
    "        old_sol[n+1] = forward_euler(old_sol[n], dt, rhs)\n",
    "    \n",
    "    # iteration loop\n",
    "    for k in range(M-1):\n",
    "        integrated_rhs = np.dot(integration_matrix, rhs(old_sol))\n",
    "        # loop over fine timesteps\n",
    "        for n in range(M-1):\n",
    "            # SDC algorithm with explicit Euler scheme as base integrator\n",
    "            ##### Enter interation here - can be done in one line!  \n",
    "        old_sol = new_sol\n",
    "\n",
    "    return new_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the function that loops over the entire time domain, linking together the above iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdc(y0, t0, tmax, Dt, M, rhs):\n",
    "    # y0 is the initial condition\n",
    "    # t0 and tmax are the start and end times\n",
    "    # Dt is the coarse timestep\n",
    "    # M is the number of fine nodes per coarse step\n",
    "    # rhs is the function returning the rhs of the equation\n",
    "\n",
    "    grid_coarse = np.arange(t0, tmax + 0.5*Dt, Dt)\n",
    "    \n",
    "    L = len(grid_coarse)\n",
    "    solution = np.empty(L, float)\n",
    "    solution[0] = y0\n",
    "    \n",
    "    for n in range(L-1):\n",
    "        #### use the sdc fine step function here!\n",
    "    \n",
    "    return grid_coarse, solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Fill in the missing lines of code!\n",
    "2. Take $y_0=1$ and run from $t=0$ to $t=5$ with a coarse timestep of 0.5. Compare the solution to the exact solution.\n",
    "3. Run with a range of coarse timesteps and confirm, using the exact solution, that each iteration adds an order of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. See this paper for the pseudocode for RIDC: https://mathgeek.us/research/papers/ridc.pdf Can you amend your code to implement this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
